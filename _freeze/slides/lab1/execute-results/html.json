{
  "hash": "20f726a0e6bb56ea8b3c06362a4f4178",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Maximum likelihood estimators\"\n# date: \"September 11, 2023\"\nformat: \n    revealjs:\n      smaller: true\n---\n\n\n\n# Background: likelihoods\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n## Example: normal likelihood\n\nLet $X$ be the resting heart rate (RHR) in beats per minute of a student in this class.\n\nAssume RHR is normally distributed with some mean $\\mu$ and standard deviation $8$.\n\n::: {.fragment}\n\n$$\n\\textbf{Data-generative model: } X_i \\overset{\\mathrm{iid}}{\\sim} N(\\mu, 64)\n$$\n:::\n\n::: {.fragment}\n\nIf we observe three student heart rates, {75, 58, 68} then our likelihood \n$$L(\\mu) = f_x(75 |\\mu) \\cdot f_x(58|\\mu) \\cdot f_x(68|\\mu).$$\nThat is, the joint density function of the observed data, viewed as a function of the parameter.\n\n:::\n\n::: {.fragment}\n\n::: callout-important\nThe likelihood itself is **not a density function** of the unknown parameter(s). The integral with respect to the parameter does not need to equal 1.\n:::\n\n::: callout-note \n## Definition\nThe **likelihood** function is the joint density (or mass function) of the data *viewed as a function of the unknown parameter(s)*.\n:::\n\n:::\n\n## Visualizing the likelihood\n\n$$L(\\mu) = f_x(75 |\\mu) \\cdot f_x(58|\\mu) \\cdot f_x(68|\\mu).$$\nThe maximum likelihood estimate $\\hat{\\mu} = \\frac{75 + 58 + 68}{3} = 67$.\n\nThe **maximum likelihood estimate** is the parameter value that *maximizes* the likelihood function. \n\n::: panel-tabset\n\n### data\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx = c(75, 58, 68)\n```\n:::\n\n\n\n\n### likelihood function\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nL = function(mu, x) {\n  stopifnot(is.numeric(x))\n  n = length(x)\n  likelihood = 1\n  for(i in 1:n){\n    likelihood = likelihood * dnorm(x[i], mean = mu, sd = 8)\n  }\n  return(likelihood)\n}\n```\n:::\n\n\n\n### plot\n\n\n\n::: {.cell layoutWidth='25'}\n::: {.cell-output-display}\n![](lab1_files/figure-revealjs/unnamed-chunk-4-1.png){width=960}\n:::\n:::\n\n\n\n### plot code\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot() +\n  xlim(c(50, 83)) +\n  geom_function(fun = L, args = list(x = x)) +\n  theme_bw() +\n  labs(x = expression(mu), y = \"likelihood\") + \n  geom_vline(xintercept = 67, color = 'red')\n```\n:::\n\n\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n\n## The log-likelihood\n\nNotice how small the y-axis is on the previous slide. What happens to the scale of the likelihood as we add additional data points?\n\n\n$$\nL(\\mu) = \\prod_{i = 1}^{n} f_x(x_i |\\mu)\n$$\n\n. . . \n\nSince densities often evaluate between 0 and 1, multiplying many together (as we usually do in likelihoods) can quickly result in floating point underflow. That is, numbers smaller than the computer can actually represent in memory.\n\n- Note: sometimes densities evaluate to greater than 1 (e.g. `dnorm(0, 0, 0.001)`) and multiplying several together can result in *overflow*.\n\n. . .\n\n#### log to the rescue!\n\n- `log` is a monotonic function, i.e. $x > y$ implies $\\log(x) > \\log(y)$, because of this the maximum of $f$ is the same as the maximum of $\\log f$.\n\n- additionally, `log` turns products into sums\n\nin practice, we always work with the log-likelihood,\n\n$$\n\\log L(\\mu) = \\sum_{i = 1}^n \\log f_x(x_i | \\mu).\n$$\n\n# MLE\n\n## Maximum likelihood estimation (MLE)\n\nHow did we know to take the average of the values to find the maximum likelihood estimator $\\hat{\\mu}$?\n\n. . .\n\nFrom calculus, we know that to maximize a univariate function, we need to find where the slope equals zero (technically, to ensure we find some maxima and not a minima we need to also check that the second derivative is negative). \n\n### Example: normal likelihood\n\nFor the normal likelihood example on the previous slide, we can see visually that the function is concave.\n\n::: panel-tabset\n## Step 1\n\nTo find the maximum, take the derivative\n\n$$\n\\begin{aligned}\n\\frac{d}{d\\mu} \\log L(\\mu) &= \\sum_{i}\\frac{d}{d\\mu} \\log f_x(x_i |\\mu)\\\\ \n&= \\sum_{i}\\frac{d}{d\\mu} \\left[ -\\frac{1}{2} \\log (2 \\pi \\sigma^2) - \\frac{1}{2\\sigma^2} (x_i - \\mu)^2 \\right]\\\\\n&= \\sum_i \\frac{1}{\\sigma^2} (x_i - \\mu)\n\\end{aligned}\n$$\n\n## Step 2\n\nSet the derivative equal to zero and solve,\n\n$$\n\\begin{aligned}\n\\sum_i \\left( x_i - \\hat{\\mu} \\right) &= 0\\\\\nn \\hat{\\mu} &= \\sum_i x_i\\\\\n\\hat{\\mu} &= \\bar{x}\n\\end{aligned}\n$$\n:::\n\n## Exercise 1: binomial MLE\n\nLet $Y_1, Y_2,  \\ldots, Y_n \\sim \\text{iid } \\text{binary}(\\theta)$. Here $\\theta$ is the probability of a success, i.e. $prob(Y_i = 1)$. \n\n- Note: a \"binary\" distribution is equivalent to a \"Bernoulli\" distribution. Your book calls it \"binary\", so we will as well for consistency.\n\na. Write down the likelihood, $p(y_1, \\ldots, y_n | \\theta)$.\n\nb. Write down the log-likelihood.\n\nc. Compute $\\hat{\\theta}_{MLE} = \\text{argmax}_{\\theta}~ \\log p(y_1, \\ldots, y_n | \\theta)$\n\n## Solution 1\n\n::: panel-tabset\n## solution 1a\n\nThe likelihood\n\n$$\n\\begin{aligned}\np(y_1, \\ldots, y_n | \\theta) &= \\prod_{i=1}^n p(y_i | \\theta)\\\\\n&= \\theta^{y_i}(1-\\theta)^{1-y_i}\\\\\n&= \\theta^{\\sum {y_i}}(1-\\theta)^{n - \\sum y_i}\n\\end{aligned}\n$$\n\n## solution 1b\n\nThe log-likelihood\n\n$$\n\\log \\left(\\theta^{\\sum {y_i}}(1-\\theta)^{n - \\sum y_i} \\right) = \nn \\bar{y} \\log \\theta + n(1-\\bar{y}) \\log(1 - \\theta)\n$$\n\nwhere $\\bar{y} = \\frac{1}{n} \\sum_{i=1}^n y_i$.\n\n## solution 1c\n\nTake the derivative \n$$\n\\begin{aligned}\n\\frac{d}{d\\theta} \\log p(y_1, \\ldots, y_n | \\theta) &=\n\\frac{n\\bar{y}}{\\theta} - \\frac{n - n\\bar{y}}{1- \\theta}\n\\end{aligned}\n$$\n\nand set equal to 0. After simplifying, \n\n$$\n\\hat{\\theta}_{MLE} = \\bar{y}\n$$\n:::\n\n## Exercise 2\n\nZero-inflated models allow for frequent zero-valued observations. You observe 100 people at the beach fishing. Click \"View data\" to see the number of fish each individual catches (reported as `fishing_count`) below.\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"View data\"}\nfishing_count = c(0, 0, 0, 3, 0, 3, 5, 0, 8, 1,\n                  0, 0, 0, 0, 3, 4, 0, 4, 8, 0,\n                  0, 0, 4, 0, 0, 6, 4, 0, 4, 0,\n                  0, 0, 0, 0, 4, 8, 4, 0, 0, 0,\n                  7, 3, 0, 2, 0, 0, 7, 7, 5, 0,\n                  0, 0, 7, 6, 0, 3, 0, 0, 0, 5,\n                  4, 1, 0, 0, 0, 0, 0, 0, 0, 8,\n                  0, 5, 0, 3, 0, 0, 3, 0, 0, 2,\n                  0, 5, 0, 7, 7, 0, 1, 0, 2, 7,\n                  0, 2, 4, 0, 0, 1, 5, 0, 0, 0)\n```\n:::\n\n\n\nThe data can be described as being generated from a zero-inflated mixture model. Let be the number of fish an individual catches at the beach,\n\n$$\n\\begin{aligned}\np(Y_i = 0) &= p +(1 - p)e^{-\\lambda}\\\\\np(Y_i = y_i) &= \\frac{(1-p) \\lambda ^ {y_i} e^{-\\lambda}}{y_i!}, ~y_i = 1, 2, 3, \\ldots\n\\end{aligned}\n$$\n\nAssume observations $Y_1, \\ldots, Y_{100}$ are independent.\n\na. Write down the log-likelihood and visualize the log-likelihood as a function of $p$ while fixing $\\lambda = 5$. Repeat for $\\lambda = 4$ (on the same plot).\n\nb. Based on your plot, which value of $\\lambda$ is more likely? Assuming these $\\lambda$ are sufficiently close to the true $\\lambda$, what is (approximately) the MLE of $p$ based on your plot?\n\nc. Can you find a closed form solution for $\\hat{\\lambda}_{MLE}, \\hat{p}_{MLE}$? Why or why not?\n\n## Solution 2\n\n::: panel-tabset\n\n## a (log-likelihood)\n\n$$\n\\log \\left(p(Y_i = 0)^{57}\\prod_{i = 1}^{43}  p(Y_i = y_i) \\right) = \\\\\n 57 \\log p(Y_i = 0) + \\sum_{i = 1}^{43} \\log p(Y_i = y_i)\n$$\n\nwhere\n\n$$\n\\begin{aligned}\np(Y_i = 0) &= p + (1-p) e^{-\\lambda}\\\\\np(Y_i = y_i) &= \\frac{(1 - p)\\lambda^{y_i}e^{-\\lambda}}{y_i!}, \\ \\ y_i = 1, 2, 3, \\ldots\n\\end{aligned}\n$$\n\n## a (code)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsum(fishing_count == 0)\nfishing_subset = fishing_count[fishing_count != 0 ]\nlength(fishing_subset) # sanity check\n\nprobYzero = function(p, lambda) {\n  prob1 = log(p + ((1 - p) * exp(-lambda)))\n  return(57 * prob1)\n}\n\nlikelihood = function(p, lambda) {\n  total = 0\n  for (i in seq_along(fishing_subset)) {\n    total = total +\n      (log(1 - p) + (fishing_subset[i] * log(lambda)) - lambda) -\n      log(factorial(fishing_subset[i]))\n  }\n  return(probYzero(p, lambda) + total)\n}\nggplot() +\n  xlim(c(0.0, 1)) +\n  geom_function(fun = likelihood, args = list(lambda = 8)) +\n  geom_function(fun = likelihood,\n                args = list(lambda = 4),\n                col = \"red\") +\n  theme_bw() +\n  labs(x = \"p\", y = \"log-likelihood\")\n```\n:::\n\n\n\n## a (plot)\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](lab1_files/figure-revealjs/unnamed-chunk-10-1.png){width=960}\n:::\n:::\n\n\n\n\n## b \n\n- $\\lambda = 4$ is more likely. \n- $\\hat{p}_{MLE} \\approx 0.5$.\n\n## c\n\nTake the gradient and set equal to zero. Two equations and two unknowns, but you cannot isolate $\\hat{\\lambda}$ when taking the derivative with respect to $\\lambda$. No closed form solution, need to solve numerically, e.g. with Newton's method.\n\n\n\n:::\n\n<!-- # MAP -->\n\n<!-- ## Maximum a posteriori probability (MAP) -->\n\n<!-- In Bayesian inference, we wish to find the mode of the **posterior**, not the likelihood. -->\n\n<!-- To find the posterior mode, $\\hat{\\theta}$, we instead take the derivative of the *log-posterior*, -->\n\n<!-- $$ -->\n<!-- \\frac{d}{d\\theta} \\log p(\\theta | y) = 0 -->\n<!-- $$ -->\n\n<!-- ### Practice exercise -->\n\n<!-- As [in class](/notes/lec02-estimation.html), let -->\n\n<!-- $$ -->\n<!-- Y | \\theta \\sim \\text{binomial}(n, \\theta)\\\\ -->\n<!-- \\theta \\sim \\text{beta}(a, b) -->\n<!-- $$ -->\n\n<!-- 1. Find the closed-form solution for the posterior mode $\\hat{\\theta}$. -->\n\n<!-- 2. Recreate Figure 1 [from class](/notes/estimation1.html) using the *same data* `flips` provided below but change the prior to $\\theta \\sim \\text{beta}(2, 2)$. -->\n\n<!-- ```{r} -->\n<!-- set.seed(3) -->\n<!-- flips = rbinom(5000, size = 1, prob = 0.25) -->\n<!-- ``` -->\n\n\n<!-- 3. Add a red vertical line to each subplot that shows the MAP estimate under the prior $\\theta \\sim \\text{beta}(2, 2)$. -->\n\n",
    "supporting": [
      "lab1_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}